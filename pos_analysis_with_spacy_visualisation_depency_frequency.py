# -*- coding: utf-8 -*-
"""POS analysis with SpaCy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JgrSDAi0ZR8_IngOJOc131Entps3tKLV
"""

import spacy

# Load the SpaCy English model
nlp = spacy.load("en_core_web_sm")

# Text to analyze
text = "Locate the corner hole on roof of shipping container. This hole faces upward and is larger than the side holes, which face outward."

# Process the text with SpaCy
doc = nlp(text)

# Initialize counters for each part of speech
pos_counts = {
    "NOUN": 0,
    "VERB": 0,
    "ADJ": 0,
    "ADV": 0,
    "PRON": 0,
    "ADP": 0,
    "CONJ": 0,
    "DET": 0,
    "NUM": 0,
    "PROPN": 0,
    "PUNCT": 0,
    "SYM": 0,
    "X": 0
}

# Create lists to store each type of POS
pos_words = {pos: [] for pos in pos_counts.keys()}

# Iterate through tokens and count parts of speech
for token in doc:
    pos = token.pos_  # Part of speech
    if pos in pos_counts:
        pos_counts[pos] += 1
        pos_words[pos].append(token.text)

# Print results
print("Detailed POS Analysis:")
for pos, count in pos_counts.items():
    if count > 0:  # Only print POS types that exist in the text
        print(f"{pos}: {count} ({', '.join(pos_words[pos])})")

# Print total counts
print("\nSummary:")
print(f"Total nouns: {pos_counts['NOUN']}")
print(f"Total verbs: {pos_counts['VERB']}")
print(f"Total adjectives: {pos_counts['ADJ']}")
print(f"Total adverbs: {pos_counts['ADV']}")

import spacy
from spacy import displacy

# Load the SpaCy English model
nlp = spacy.load("en_core_web_sm")

# Text to analyze
text = "Locate the corner hole on roof of shipping container. This hole faces upward and is larger than the side holes, which face outward."

# Process the text with SpaCy
doc = nlp(text)

# 1. Detailed POS Analysis
print("Detailed POS Analysis:")
pos_counts = {
    "NOUN": 0,
    "VERB": 0,
    "ADJ": 0,
    "ADV": 0,
    "PRON": 0,
    "ADP": 0,
    "CONJ": 0,
    "DET": 0,
    "NUM": 0,
    "PROPN": 0,
    "PUNCT": 0,
    "SYM": 0,
    "X": 0
}

# Create lists to store each type of POS
pos_words = {pos: [] for pos in pos_counts.keys()}

# Iterate through tokens and count parts of speech
for token in doc:
    pos = token.pos_  # Part of speech
    if pos in pos_counts:
        pos_counts[pos] += 1
        pos_words[pos].append(token.text)

# Print Detailed POS Analysis
for pos, count in pos_counts.items():
    if count > 0:  # Only print POS types that exist in the text
        print(f"{pos}: {count} ({', '.join(pos_words[pos])})")

# Print Summary
print("\nSummary:")
print(f"Total nouns: {pos_counts['NOUN']}")
print(f"Total verbs: {pos_counts['VERB']}")
print(f"Total adjectives: {pos_counts['ADJ']}")
print(f"Total adverbs: {pos_counts['ADV']}")

# 2. Dependency Parsing Visualization
print("\nDependency Parsing Tree:")
displacy.render(doc, style="dep", jupyter=True, options={'distance': 120})

# 3. Named Entity Recognition (NER) Visualization
print("\nNamed Entities:")
displacy.render(doc, style="ent", jupyter=True)

# 4. Noun Phrase Visualization
print("\nNoun Phrases:")
for chunk in doc.noun_chunks:
    print(f"{chunk.text} -> Root: {chunk.root.text}, Head: {chunk.root.head.text}")

# Noun Phrase Visualization using Displacy
noun_phrases = [{"text": chunk.text, "start": chunk.start_char, "end": chunk.end_char, "label": "NOUN_PHRASE"} for chunk in doc.noun_chunks]
options = {"ents": ["NOUN_PHRASE"], "colors": {"NOUN_PHRASE": "lightblue"}}
displacy.render({"text": text, "ents": noun_phrases, "title": "Noun Phrases"}, style="ent", manual=True, jupyter=True, options=options)

import spacy
from spacy import displacy
from collections import Counter
import matplotlib.pyplot as plt

# Load the SpaCy English model
nlp = spacy.load("en_core_web_sm")

# Text to analyze
text = "Locate the corner hole on roof of shipping container. This hole faces upward and is larger than the side holes, which face outward."

# Process the text with SpaCy
doc = nlp(text)

# 1. POS Tagging Visualization
print("POS Tagging:")
for token in doc:
    print(f"{token.text} -> {token.pos_}")

# POS Tagging Visualization using Displacy
pos_ents = [{"text": token.text, "start": token.idx, "end": token.idx + len(token.text), "label": token.pos_} for token in doc]
options = {"ents": list(set([token.pos_ for token in doc])), "colors": {"NOUN": "lightblue", "VERB": "orange", "ADJ": "pink", "ADV": "yellow"}}
displacy.render({"text": text, "ents": pos_ents, "title": "POS Tagging"}, style="ent", manual=True, jupyter=True, options=options)

# 2. Dependency Arc Details with Labels
print("\nDependency Parsing with Labels:")
displacy.render(doc, style="dep", jupyter=True, options={'distance': 120, 'compact': False, 'color': 'purple', 'bg': 'white'})

# 3. Word Frequency Chart
word_freq = Counter([token.text.lower() for token in doc if not token.is_stop and token.is_alpha])
plt.figure(figsize=(10, 6))
plt.bar(word_freq.keys(), word_freq.values(), color='skyblue')
plt.title('Word Frequency')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 4. POS Distribution Pie Chart
pos_counts = Counter([token.pos_ for token in doc])
plt.figure(figsize=(8, 8))
plt.pie(pos_counts.values(), labels=pos_counts.keys(), autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(pos_counts))))
plt.title('POS Distribution')
plt.show()